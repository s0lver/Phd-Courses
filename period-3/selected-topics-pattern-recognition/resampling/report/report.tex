\documentclass{article}
\usepackage[spanish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage[inline]{enumitem}
\hypersetup{colorlinks=true,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{algorithm}
\usepackage{algorithmic}
\floatname{algorithm}{Algoritmo}
\renewcommand{\listalgorithmname}{Lista de algoritmos}
\renewcommand{\algorithmicrequire}{\textbf{Entrada:}}
\renewcommand{\algorithmicensure}{\textbf{Salida:}}
\renewcommand{\algorithmicend}{\textbf{fin}}
\renewcommand{\algorithmicif}{\textbf{si}}
\renewcommand{\algorithmicthen}{\textbf{entonces}}
\renewcommand{\algorithmicelse}{\textbf{si no}}
\renewcommand{\algorithmicelsif}{\algorithmicelse,\ \algorithmicif}
\renewcommand{\algorithmicendif}{\algorithmicend\ \algorithmicif}
\renewcommand{\algorithmicfor}{\textbf{para}}
\renewcommand{\algorithmicforall}{\textbf{para todo}}
\renewcommand{\algorithmicdo}{\textbf{hacer}}
\renewcommand{\algorithmicendfor}{\algorithmicend\ \algorithmicfor}
\renewcommand{\algorithmicwhile}{\textbf{mientras}}
\renewcommand{\algorithmicendwhile}{\algorithmicend\ \algorithmicwhile}
\renewcommand{\algorithmicloop}{\textbf{repetir}}
\renewcommand{\algorithmicendloop}{\algorithmicend\ \algorithmicloop}
\renewcommand{\algorithmicrepeat}{\textbf{repetir}}
\renewcommand{\algorithmicuntil}{\textbf{hasta que}}
\renewcommand{\algorithmicprint}{\textbf{imprimir}} 
\renewcommand{\algorithmicreturn}{\textbf{devolver}} 
\renewcommand{\algorithmictrue}{\textbf{cierto }} 
\renewcommand{\algorithmicfalse}{\textbf{falso }} 


\begin{document}
\title{
	\begin{figure}[!ht]
	\centering
		\includegraphics[scale=0.8]{resources/images/cinvestav-logo}
		\\[0.5cm]LTI Cinvestav Tamaulipas
	\end{figure}
	\vspace{1cm}
	Técnicas de validación y remuestreo{\small[1.e.ii-v]}
	\vspace{1cm}
}
	
\author{Rafael Pérez Torres}	
		
\date{
	Tópicos Selectos en Reconocimiento de Patrones \\ 
	\vspace{0.8cm}
	Profesor Dr. Wilfrido Gómez Flores \\
	\vspace{1cm}
	%\today
}

\maketitle
% \setlength{\parindent}{0pt}

\begin{abstract}
El presente documento presenta una descripción de distintas familias de técnicas de validación cruzada y remuestreo, en particular bootstrap.
Gracias a este tipo de mecanismos, es posible realizar una mejor definición del modelo a utilizar en la clasificación, intentado obtener los mejores resultados y asegurar la independencia de los mismos respecto a los conjuntos de entrenamiento y clasificación.
Asimismo, se presenta una breve metodología para realizar el diseño de una estrategia de clasificación utilizando tres subconjuntos: entrenamiento, validación y clasificación.
\end{abstract}

\section{Introducción}
Los métodos de remuestreo y validación cruzada resultan útiles para dos tareas en particular, la selección del modelo y la estimación del performance de los clasificadores.

En el caso de la selección del modelo, es importante destacar que todas las técnicas de clasificación cuentan con uno o más parámetros que deben ser adaptados.
Por ejemplo, el número de vecinos en un clasificador k-nn o el tamaño de la red, parámetros de aprendizaje y pesos en una red neuronal.
¿Cómo poder adaptar dichos parámetros?
En el caso de la estimación del performance, es importante destacar que dicha medida debería ser realizada cuando se aplica el clasificador a la población total.
¿Cómo estimar dicho performance?

Si se tuviera acceso a un número ilimitado de muestras, ambas preguntas tendrían una respuesta inmediata: bastaría elegir el modelo que provea la tasa de error más baja en la población completa, y que dicha tasa de error sea igual al valor verdadero.
Sin embargo, en el mundo real solamente se tiene acceso a un conjunto finito de muestras que es usualmente menor a la cantidad que se desearñia.
Un enfoque aplicable sería utilizar el conjunto de datos completo para seleccionar al clasificador y estimar el error. Sin embargo, este enfoque tiene dos problemas fundamentales:
\begin{itemize}
	\item El modelo final estaría sobreentrenado, lo cual se hace evidente en modelos con un número grande de parámetros. En la práctica el clasificador fallaría al asignar las etiquetas a las muestras que \emph{no vio} durante la etapa de entrenamiento.
	\item El error estimado sería marcadamente optimista (menor que el error real).
\end{itemize}


En general, las técnicas de remuestreo y validación cruzada intentar dar respuesta a este problema a través de mecanismos para realizar la división del conjunto de  datos.
Por un lado, las técnicas de validación cruzada dividen el conjunto de datos en partes que son independientes una de las otras; en otras palabras, no existen elementos repetidos en cada una de dichas partes.
Por otro lado, las técnicas de remuestreo permiten crear particiones en las que los elementos son seleccionados \emph{con reemplazo}, pudiendo aparecer más de una ocasión tanto en el conjunto de entrenamiento como en el de prueba.

El sobreentrenamiento o sobre-ajuste ocurre cuando el tamaño de los datos de entrenamiento es sumamente pequeño o cuando el número de parámetros del modelo es grande, y en general se refiere al hecho de que el clasificador es incapaz de reconocer correctamente muestras que \emph{no haya visto} durante la etapa de entrenamiento.

En base a las particularidades de la división del conjunto de datos, es posible estimar la magnitud del error a obtener por el clasificador.
Habitualmente, la selección de los conjuntos y el entrenamiento y clasificación asociados, se realiza en varias intervenciones, obteniendo la media de los resultados como el valor final del error.


\section{Técnicas de validación cruzada}
Las técnicas de validación cruzada básicamente dividen al conjunto de datos, de tal manera que cada una de las particiones contienen elementos únicos y cada elemento puede aparecer únicamente en una sola partición.
La división es aleatoria obteniendo dos conjuntos, uno que se utiliza como el clásico conjunto de entrenamiento, y el otro -- de validación -- es utilizado para estimar el error de generalización del conjunto de validación.
Dado que el objetivo final en el diseño de un clasificador es alcanzar un error bajo de generalización, se entrena al clasificador hasta que se obtiene un valor mínimo de este error.
La validación cruzada puede ser aplicada a virtualmente cada método de clasificación, con la salvedad de adaptar el método de validación a las particularidades de cada técnica.

% \begin{figure}
% 	\centering
% 	\includegraphics[width=\textwidth]{resources/images/validacion-cruzada-general}
% 	\caption{División básica de los métodos de validación cruzada}
% 	\label{fig:validacion-cruzada-general}
% \end{figure}

En general, la validación cruzada sólo produce resultados significativos si el conjunto de validación y prueba de conjunto se han extraído de la misma población.
Las siguientes son las técnicas de validación cruzada más utilizadas.


\subsection{Holdout validation}
El método holdout, algunas veces llamado estimación de muestra de prueba, divide a los datos en dos subconjuntos mutuamente exclusivos, denominados conjunto de entrenamiento y conjunto de prueba (o conjunto holdout).
Comúnmente, se designa $2/3$ del conjunto de datos para el entrenamiento y el $1/3$ restante para estimar el error del clasificador entrenado, como se muestre en la Figura~\ref{fig:holdout-validation}.
\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{resources/images/holdout}
	\caption{Holdout validation}
	\label{fig:holdout-validation}
\end{figure}


Las principales desventajas del método holdout son referentes a que no en todos los problemas se cuenta con una cantidad suficiente de instancias para poder aislar un subconjunto especial para la prueba.
Además, dado que su naturaleza es de un sólo experimento de entrenamiento y prueba, la estimación del error tiende a ser no representativa.
En última instancia, la técnica holdout hace un uso ineficiente de los datos, ya que casi un tercio del dataset no es utilizado para entrenar el clasificador.
La mayor cantidad de instancias destinadas al conjunto de prueba incrementan el bias de la estimación.


\subsection{K-­fold cross validation}
En la validación cruzada K-fold, también llamada estimación de rotación, el conjunto de datos se divide aleatoriamente en $k$ particiones mutuamente excluyentes.
La ejecución del clasificador es realizada $k$ veces en los tiempos $t \in \left \{ 1,2,\ldots,k \right \} $.
Por cada experimento realizado en el tiempo $t$, se utilizan $k-1$ particiones para el entrenamiento y la restante se destina para la prueba, como se muestra en la Figura~\ref{fig:k-fold}
\begin{figure}
	\centering
	\includegraphics[width=0.75\textwidth]{resources/images/k-fold}
	\caption{K-fold cross validation}
	\label{fig:k-fold}
\end{figure}

La principal ventaja de la validación cruzada k-fold es que todos los ejemplos del dataset son eventuelmente utilizados para entrenamieto y prueba.
El error real puede ser estimado como el error promedio:
$$
	E = \frac{1}{k} \sum_{i=1}^k E_{i}
$$

\subsubsection*{Cuántas particiones se necesitan?}
Un número grande de particiones arrojaría los siguientes resultados:
\begin{itemize}
	\item \checkmark El bias del estimador del error real sería pequeño.
	\item \texttimes La varianza del estimador del error real sería grande.
	\item \texttimes El tiempo de cómputo sería muy grande (muchos experimentos).
\end{itemize}

Un número pequeño de particiones obtendría:
\begin{itemize}
	\item \checkmark El número de experimentos y tiempo computacional sería reducido.
	\item \checkmark La varianza del estimador del error real sería pequeña.
	\item \texttimes El bias del estimador sería grande
\end{itemize}

En la práctica, la elección del número de particiones depende del tamaño del dataset.
Para datasets grandes, incluso 3-fold cross validation sería preciso.
Para datasets dispersos, es aconsejable utilizar leave-one-out para entrenar con la mayor cantidad de ejemplos posibles. Una elección comúm para k-fold cross validation es $k=10$.


\subsection{Leave-­one-out cross validation (LOOCV)}
Es un caso especial de k-fold cross validation, cuando se indica que K sea el número total de ejemplos.
Para un dataset de $n$ muestras, se desarrollan $n$ experimentos.
En cada experimento, se utilizan entonces $n-1$ instancias para entrenamiento y la restante para la prueba, tal como se muestra en la Figura 
Esta validación cruzada indica separar los datos de tal forma que se tenga una sola muestra para el subconjunto de de prueba y el resto de datos para el subconjunto de entrenamiento, tal como se muestra en la Figura~\ref{fig:leave-one-out-validation}.
\begin{figure}
	\centering
	\includegraphics[width=0.75\textwidth]{resources/images/leave-one-out}
	\caption{Leave-one-out cross validation}
	\label{fig:leave-one-out-validation}
\end{figure}

El error real puede ser estimado como el error promedio obtenido en los ejemplos de prueba:
$$
	E = \frac{1}{n} \sum_{i=1}^n E_{i}
$$


\section{Bootstrap, bootstrap .632 y bootstrap .632+}
De forma general, la familia de métodos \emph{bootstrap} son técnicas de remuestreo en las que de un dataset conformado por $n$ muestras se eligen $n$, con la característica distintiva de que la selección es con reemplazo, permitiendo que cada instancia aparezca múltiples veces en el conjunto originado.
Esta familia de técnicas es sumamente útil cuando el conjunto de datos es pequeño, permitiendo generar conjuntos sintéticos a partir del dataset original para las tareas de entrenamiento y clasificación.

Nuevamente, de forma general, para realizar la estimación del error se procede a realizar la selección de N subconjuntos bootstrap y procesar los resultados de error obtenidos.
En la práctica, la repetición de estas operaciones funciona dado que, si las muestras son elegidas de forma aleatoria, mantendrán las mismas características de la población en la que fueron originadas.

\subsection{Cálculo de la estimación del error en la familia de bootstrap} 
Si el dataset de entrenamiento es $\mathbf{X} = (x_1,\ldots,x_N)$ entonces es posible tomas $\mathcal{B}$ muestras bootstrap con reemplazo denominadas $\mathbf{Z_1}, \ldots, \mathbf{Z_B}$ donde cada $\mathbf{Z_i}$ contiene $n$ instancias.
Entonces es posible utilizar al conjunto de muestras $\mathbf{Z}$ para estimar el error real en la predicción del clasificador como:
$$
\text{Err}_{\text{boot}} = \frac{1}{B} \sum_{b=1}^{B} \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(y_i, f^b(x_i))
$$
donde $f^b(x_i)$ es el valor predecido para $x_i$ según el modelo generado para el $i$-ésimo dataset bootstrap y $\mathcal{L}$ es una función para medir el error del clasificador.

Sin embargo, este no es un buen estimador del error porque las muestras utilizadas para el entrenamiento del clasificador pudieron haber contenido a $x_i$.
El estimador leave-one-out bootstrap ofrece una mejora que intenta imitar al enfoque de cross-validation y se define como:
$$
\text{Err}_{\text{boot(1)}} = \frac{1}{N} \sum_{i=1}^N \frac{1}{|C^{-1}|} \sum_{b \in C^{-i}} \mathcal{L}(y_i, f^b(x_i))
$$
donde $C^{-i}$ es el conjunto de índices para los que las conjuntos bootstrap no incluyen la observación $i$ y $|C^{-i}|$ es la cantidad de dichos conjuntos.

$\text{Err}_{\text{boot(1)}}$ soluciona el problema de sobreajuste, pero continua manteniendo un bias originado por las observaciones no distintas originadas por el muestreo con reemplazamiento.
Se ha calculado, que el promedio de instancias distintas en cada conjunto bootstrap es aproximadamente de $0.632N$.
Efron y Tibshirani propusieron el estimador $0.632$ definido como:
\begin{equation}
	\text{Err}_{.632} = 0.368 \overline{\text{err}} + 0.632 \text{Err}_{\text{boot}(1)}
	\label{eq:bootstrap}
\end{equation}
donde
$$
\overline{\text{err}} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(y_i,f^b(x_i))
$$
La formulación de la ecuación \ref{eq:bootstrap} puede ser generalizada como:
\begin{equation}
	\text{Err}_{.632} = w \overline{\text{err}} + (1 - w) \text{Err}_{\text{boot}(1)}
\end{equation}
donde $w=0.368$.


Bajo algunos escenarios, el método bootstrap .632 puede seguir describiendo un sobreajuste grande, que puede ser medido por $\text{Err}_{\text{boot}(1)} - \overline{\text{err}}$.
Típicamente, esto sucede cuando el error de resustitución ($\text{Err}_{\text{boot}(1)}$) es $0$.
Para solucionar esto, el estimador \emph{bootstrap .632+} pone un peso $w$ mayor en $\text{Err}_{\text{boot}(1)}$, calculado a partir de \emph{la tasa de traslape relativo} $\hat{R}$ mediante
$$
\hat{R} = \frac{\overline{\text{err}} - \text{Err}_{\text{boot}(1)}}{\hat{\gamma} - \text{Err}_{\text{boot}(1)}}
$$
donde $\hat{\gamma}$ es la tasa de error de no-información, estimada mediante la permutación de las respuestas $y_i$ y los predictores (patrones) $t_j$.
Para un problema de clasificación dicotómico dicha tasa puede ser calculada de la siguiente manera.
Sea $\hat{p}_1$ la proporción de respuestas $y_i = 1$  y $\hat{q}_1$ la proporción de predicciones observadas $r_x(t_j) = 1$, entonces:
\begin{equation}
	\hat{\gamma} = \hat{p}_1(1-\hat{q}_1) + (1-\hat{p}_1) \hat{q}_1
\end{equation}

Finalmente, la estimación del error bootstrap .632+ es definida como:
\begin{equation}
	\text{Err}_{.632+} = w \overline{\text{err}} + (1 - w) \text{Err}_{\text{boot}(1)}, \text{ } w = \frac{.632}{1-.368\hat{R}}
\end{equation}
El estimador bootstrap .632+ es sumamente confiable cuando $n>p$ (la cantidad de instancias es mayor a la cantidad de atributos o dimensiones).

\section{División de datos en tres conjuntos}
Normalmente, si la selección del modelo y las estimaciones del error real van a ser calculadas simultáneamente, los datos necesitan ser divididos en tres conjuntos disjuntos.
\begin{itemize}
	\item \textbf{Entrenamiento}: Un conjunto de ejemplos utilizados para aprendizaje, en específico para ajustar los parámetros del clasificador.
	\item \textbf{Validación}: Un conjunto de instancias utilizadas para adaptar los parámetros del clasificador.
	\item \textbf{Prueba}: Un conjunto de ejemplos utilizados únicamente para evaluar el desempeño de un clasificador compltamente entrenado. Después de esta evaluación, el modelo ya no debe ser modificado.
\end{itemize}

El procedimiento para su ejecución es mostrado en el Algoritmo~\ref{alg:algoritmo}. 
Dicho algoritmo asume un métdo holdout, en caso de utilizar validación cruzada o bootstrap, los pasos 3 y 4 deben ser repetidos por cada una de las $k$ particiones. 

\begin{algorithm} 
\begin{algorithmic}[1] 
\STATE Dividir el conjunto de datos disponible en subconjuntos de entrenamiento, validación y prueba.
\STATE Seleccionar una arquitectura de clasificador y parámetros de entrenamiento\label{paso-2}.
\STATE Entrenar el modelo a través del conjunto de entrenamiento. 
\STATE Evaluar el modelo utilizando el conjunto de validación. \label{paso-4}
\STATE Repetir los pasos \ref{paso-2} al \ref{paso-4} utilizando distintas arquitecturas y parámetros de entrenamiento.
\STATE Seleccionar el mejor modelo y entrenarlo utilizando los datos de los conjuntos de entrenamiento y validación.
\STATE Evaluar el desempeño de este modelo final utilizando el conjunto de prueba.
\end{algorithmic} 
\caption{Algoritmo para realizar entrenamiento, validación y prueba de un clasificador} 
\label{alg:algoritmo}
\end{algorithm}
\newpage 
\nocite{*}
\bibliographystyle{plain}
\bibliography{references} 

\end{document}

% http://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works?rq=1
% http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html
% https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29
% http://linus.nci.nih.gov/techreport/prederr_rev_0407.pdf
% http://www.stat.rutgers.edu/home/mxie/rcpapers/bootstrap.pdf
% http://ai.stanford.edu/~ronnyk/accEst.pdf
% http://stats.stackexchange.com/questions/96739/what-is-the-632-rule-in-bootstrapping
% http://www.stat.cmu.edu/~brian/724/week11/lec27-bootstrap.pdf
% https://es.wikipedia.org/wiki/Validaci%C3%B3n_cruzada
% http://genome.tugraz.at/proclassify/help/pages/XV.html
% http://courses.cs.tamu.edu/rgutier/ceg499_s02/l13.pdf

% Libros:
% The elements of statistical learning, pág 271
% Pattern recognition, theodoris, 581
% Pattern classification, duda, 562